{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaD3M5HhjFrXLH72NOQ6dZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonjloeffler-lab/Semester-Project/blob/main/collect_data.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQSqWv4j7oHJ",
        "outputId": "20abc647-beef-4a6f-c00d-8c0850ab61fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting initial data collection from 2020 to 2025...\n",
            "Successfully collected 69 historical records and saved to data/bls_data.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "BLS_API_KEY = os.getenv(\"BLS_API_KEY\", \"88317efad228417fb8b93e6d0796cb8e\")\n",
        "BLS_API_URL = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n",
        "\n",
        "SERIES_MAP = {\n",
        "    \"LNS14000000\": \"Unemployment_Rate_SA\",\n",
        "    \"CES0000000001\": \"Total_Nonfarm_Employment_SA\",\n",
        "    \"CES0500000003\": \"Avg_Weekly_Hours_Private_SA\",\n",
        "    \"PRS85006092\": \"Output_Per_Hour_NF\",\n",
        "    \"CUUR0000SA0L1E\": \"CPI_U_Ex_Food_Energy_U\",\n",
        "    \"EIUIR\": \"Imports_All_Commodities_U\",\n",
        "    \"EIUIQ\": \"Exports_All_Commodities_U\",\n",
        "}\n",
        "\n",
        "# Define the data ranges\n",
        "FULL_HISTORY_YEARS = 5\n",
        "UPDATE_WINDOW_YEARS = 3 # Fetch data for the last 3 years on monthly updates\n",
        "\n",
        "END_YEAR = datetime.now().year\n",
        "START_YEAR = END_YEAR - FULL_HISTORY_YEARS\n",
        "DATA_FILE_PATH = \"data/bls_data.csv\"\n",
        "\n",
        "# --- BLS API Fetch Function (No changes needed here) ---\n",
        "def get_bls_data(series_ids, start_year, end_year):\n",
        "    \"\"\"Fetches data from the BLS API for the given series and date range.\"\"\"\n",
        "    '''\n",
        "    # Check if API key is set before making the request\n",
        "    if BLS_API_KEY == \"88317efad228417fb8b93e6d0796cb8e\" or not BLS_API_KEY:\n",
        "        print(\"API Key not found. Please set the BLS_API_KEY environment variable.\")\n",
        "        return None\n",
        "    '''\n",
        "    headers = {'Content-type': 'application/json'}\n",
        "    data = {\n",
        "        \"seriesid\": series_ids,\n",
        "        \"startyear\": str(start_year),\n",
        "        \"endyear\": str(end_year),\n",
        "        \"registrationkey\": BLS_API_KEY,\n",
        "        \"catalog\": False,\n",
        "        \"calculations\": False,\n",
        "        \"annualaverage\": False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(BLS_API_URL, headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "        json_data = response.json()\n",
        "\n",
        "        if json_data.get('status', '').strip() == 'REQUEST_SUCCEEDED':\n",
        "\n",
        "            return json_data['Results']['series']\n",
        "        else:\n",
        "            print(f\"BLS API Error: {json_data.get('message', 'Unknown Error')}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred during API request: {e}\")\n",
        "        return None\n",
        "\n",
        "#  Data Processing\n",
        "def process_data(series_results):\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for series in series_results:\n",
        "        series_id = series['seriesID']\n",
        "        column_name = SERIES_MAP.get(series_id, series_id)\n",
        "\n",
        "        for item in series['data']:\n",
        "            year = item['year']\n",
        "            period = item['period']\n",
        "            value = item['value']\n",
        "\n",
        "            date_str = None\n",
        "            if period.startswith('M'):\n",
        "                month = int(period[1:])\n",
        "                date_str = f\"{year}-{month:02d}-01\"\n",
        "            elif period.startswith('Q'):\n",
        "                # Map quarterly data to the last month of the quarter\n",
        "                quarter_map = {'Q01': 3, 'Q02': 6, 'Q03': 9, 'Q04': 12}\n",
        "                month = quarter_map.get(period, None)\n",
        "                if month is not None:\n",
        "                    date_str = f\"{year}-{month:02d}-01\"\n",
        "\n",
        "            if date_str:\n",
        "                if date_str not in processed_data:\n",
        "                    processed_data[date_str] = {}\n",
        "\n",
        "                try:\n",
        "                    processed_data[date_str][column_name] = float(value)\n",
        "                except ValueError:\n",
        "                    processed_data[date_str][column_name] = None\n",
        "\n",
        "    df = pd.DataFrame.from_dict(processed_data, orient='index')\n",
        "    df.index.name = 'Date'\n",
        "    df = df.reset_index()\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "#  1. Initial Data Collection\n",
        "def initial_data_collection():\n",
        "    \"\"\"Fetches full history (5 years) and saves the initial CSV.\"\"\"\n",
        "    print(f\"Starting initial data collection from {START_YEAR} to {END_YEAR}...\")\n",
        "\n",
        "    series_ids = list(SERIES_MAP.keys())\n",
        "    series_data = get_bls_data(series_ids, START_YEAR, END_YEAR)\n",
        "\n",
        "    if series_data:\n",
        "        df_final = process_data(series_data)\n",
        "\n",
        "        os.makedirs(os.path.dirname(DATA_FILE_PATH), exist_ok=True)\n",
        "\n",
        "        df_final.to_csv(DATA_FILE_PATH, index=False)\n",
        "        print(f\"Successfully collected {len(df_final)} historical records and saved to {DATA_FILE_PATH}\")\n",
        "    else:\n",
        "        print(\"Initial data collection failed.\")\n",
        "\n",
        "# 2. Monthly Data Update (GitHub Action)\n",
        "def update_data_and_save():\n",
        "    \"\"\"Fetches the latest data (last 3 years), merges it with existing data, and saves.\"\"\"\n",
        "\n",
        "    # 1. Determine the date range for the update\n",
        "    update_start_year = datetime.now().year - UPDATE_WINDOW_YEARS\n",
        "    update_end_year = datetime.now().year\n",
        "\n",
        "    print(f\"Fetching data for update from {update_start_year} to {update_end_year}...\")\n",
        "\n",
        "    series_ids = list(SERIES_MAP.keys())\n",
        "    series_data = get_bls_data(series_ids, update_start_year, update_end_year)\n",
        "\n",
        "    if not series_data:\n",
        "        print(\"Monthly update data collection failed.\")\n",
        "        return\n",
        "\n",
        "    # 2. Process the new data\n",
        "    df_new = process_data(series_data)\n",
        "\n",
        "    # 3. Read the existing data\n",
        "    df_existing = pd.read_csv(DATA_FILE_PATH, parse_dates=['Date'])\n",
        "    print(f\"Loaded existing data with {len(df_existing)} records.\")\n",
        "\n",
        "    # 4. Combine data and drop duplicates (based on the Date column)\n",
        "\n",
        "    df_combined = pd.concat([df_existing, df_new]).drop_duplicates(subset=['Date'], keep='last')\n",
        "\n",
        "    # 5. Final cleanup and save\n",
        "    df_combined = df_combined.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "    # Check if any new records were actually added/updated\n",
        "    if len(df_combined) > len(df_existing):\n",
        "        print(f\"New data found! {len(df_combined) - len(df_existing)} new record(s) appended.\")\n",
        "    else:\n",
        "        print(\"No new records found, but potential revisions were saved.\")\n",
        "\n",
        "    df_combined.to_csv(DATA_FILE_PATH, index=False)\n",
        "    print(f\"Data updated successfully. Total records now: {len(df_combined)}\")\n",
        "\n",
        "\n",
        "# Main Execution Logic\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure data directory exists\n",
        "    os.makedirs(os.path.dirname(DATA_FILE_PATH), exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(DATA_FILE_PATH):\n",
        "        # RUN 1: File doesn't exist, run initial historical collection\n",
        "        initial_data_collection()\n",
        "    else:\n",
        "        # RUN 2+: File exists, run monthly update logic\n",
        "        update_data_and_save()"
      ]
    }
  ]
}